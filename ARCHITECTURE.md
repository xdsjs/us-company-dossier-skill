# US Company Dossier Skill - æŠ€æœ¯æ¶æ„æ–‡æ¡£

## æ¦‚è¿°

US Company Dossier Skill æ˜¯ä¸€ä¸ªä¼ä¸šçº§çš„å…¬å¸æ•°æ®é‡‡é›†å’Œå¤„ç†ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºä» SEC EDGAR å’ŒæŠ•èµ„è€…å…³ç³»ç½‘ç«™è‡ªåŠ¨æ„å»ºå¯å®¡è®¡ã€å¯å¢é‡æ›´æ–°ã€å¯æ£€ç´¢çš„å…¬å¸ç ”ç©¶æ¡£æ¡ˆã€‚

**ç‰ˆæœ¬**: 2.0  
**æŠ€æœ¯æ ˆ**: Python 3.8+, Requests, html2text, dataclasses  
**è®¾è®¡ç›®æ ‡**: å¯è¿½æº¯æ€§ã€åˆè§„æ€§ã€å¯æ‰©å±•æ€§ã€çµæ´»æ€§  
**æ–°ç‰¹æ€§**: æ”¯æŒ `links_only` å’Œ `full` ä¸¤ç§ä¸‹è½½æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨è½»é‡çº§ `links_only` æ¨¡å¼

---

## ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph UserInterface[ç”¨æˆ·æ¥å£å±‚]
        CLI[CLIå‘½ä»¤è¡Œ]
        PythonAPI[Python API]
    end
    
    subgraph CoreEngine[æ ¸å¿ƒå¼•æ“]
        BuildDossier[BuildDossier<br/>ä¸»æ§åˆ¶å™¨]
        CompanyInfo[CompanyInfo<br/>å…¬å¸ä¿¡æ¯]
        FileDownload[FileDownload<br/>æ–‡ä»¶ä¸‹è½½]
        Normalize[Normalize<br/>æ–‡æ¡£è§„èŒƒåŒ–]
        Index[Index<br/>ç´¢å¼•ç”Ÿæˆ]
    end
    
    subgraph DataSources[æ•°æ®æº]
        SECAPI[SEC EDGAR API]
        IRSites[IRç½‘ç«™]
    end
    
    subgraph Storage[å­˜å‚¨å±‚]
        RawFiles[Raw Files<br/>åŸå§‹æ–‡ä»¶]
        NormalizedFiles[Normalized Files<br/>è§„èŒƒåŒ–æ–‡ä»¶]
        IndexDB[Index JSONL<br/>ç´¢å¼•æ•°æ®åº“]
        Manifest[Manifest JSON<br/>å…ƒæ•°æ®æ¸…å•]
    end
    
    CLI --> BuildDossier
    PythonAPI --> BuildDossier
    
    BuildDossier --> CompanyInfo
    BuildDossier --> FileDownload
    BuildDossier --> Normalize
    BuildDossier --> Index
    
    CompanyInfo --> SECAPI
    FileDownload --> SECAPI
    FileDownload --> IRSites
    
    FileDownload --> RawFiles
    Normalize --> RawFiles
    Normalize --> NormalizedFiles
    Index --> NormalizedFiles
    Index --> IndexDB
    BuildDossier --> Manifest
```

### æ ¸å¿ƒæ¨¡å—

| æ¨¡å— | èŒè´£ | ä¸»è¦ç±»/å‡½æ•° |
|------|------|------------|
| **æ•°æ®æ¨¡å‹** | å®šä¹‰æ•°æ®ç»“æ„ | `Artifact`, `CompanyInfo`, `RunInfo` |
| **SEC é›†æˆ** | ä¸ SEC API äº¤äº’ | `_get_company_info()`, `_fetch_sec_filings()` |
| **æ–‡ä»¶ä¸‹è½½** | ä¸‹è½½å’Œç¼“å­˜æ–‡ä»¶ | `_download_sec_filings_optimized()` |
| **æ–‡æ¡£å¤„ç†** | HTML è½¬ Markdown | `_normalize_artifact()` |
| **ç´¢å¼•ç³»ç»Ÿ** | ç”Ÿæˆå¯æœç´¢ç´¢å¼• | `_chunk_document()`, `_normalize_and_index()` |
| **è´¨é‡æ§åˆ¶** | éªŒè¯æ•°æ®å®Œæ•´æ€§ | `_calculate_quality_metrics()` |

---

## æ•°æ®æµæ¶æ„

### å®Œæ•´æ•°æ®æµ

```mermaid
sequenceDiagram
    participant User
    participant BuildDossier
    participant SEC_API
    participant FileSystem
    participant Normalizer
    participant Indexer
    
    User->>BuildDossier: build_dossier(ticker="TSLA", download_mode="links_only")
    
    Note over BuildDossier: 1. åˆå§‹åŒ–
    BuildDossier->>FileSystem: åˆ›å»º dossier ç›®å½•
    BuildDossier->>FileSystem: åˆå§‹åŒ– manifest.json
    
    Note over BuildDossier: 2. è·å–å…¬å¸ä¿¡æ¯
    BuildDossier->>SEC_API: æŸ¥è¯¢ Ticker -> CIK
    SEC_API-->>BuildDossier: CIK: 0001318605
    
    Note over BuildDossier: 3. è·å–æ–‡ä»¶åˆ—è¡¨
    BuildDossier->>SEC_API: è·å– filings å…ƒæ•°æ®
    SEC_API-->>BuildDossier: Filing åˆ—è¡¨ (10-K, 10-Q, 8-K...)
    
    Note over BuildDossier: 4. å¤„ç†æ–‡ä»¶ï¼ˆæ ¹æ®æ¨¡å¼ï¼‰
    alt download_mode = "links_only"
        loop æ¯ä¸ª filing
            BuildDossier->>BuildDossier: ç”Ÿæˆ SEC åœ¨çº¿æŸ¥çœ‹å™¨ URL
            BuildDossier->>FileSystem: "ä¿å­˜å…ƒæ•°æ®åˆ° manifestï¼ˆä¸ä¸‹è½½æ–‡ä»¶ï¼‰"
        end
    else download_mode = "full"
        loop æ¯ä¸ª filing
            BuildDossier->>SEC_API: ä¸‹è½½ filing HTML
            SEC_API-->>BuildDossier: HTML å†…å®¹
            BuildDossier->>FileSystem: ä¿å­˜åˆ° raw/sec/
        end
        
        Note over BuildDossier: 5. è§„èŒƒåŒ–å¤„ç†ï¼ˆä»… full æ¨¡å¼ï¼‰
        loop æ¯ä¸ª artifact
            BuildDossier->>Normalizer: è½¬æ¢ HTML -> Markdown
            Normalizer->>FileSystem: ä¿å­˜åˆ° normalized/
        end
        
        Note over BuildDossier: 6. ç”Ÿæˆç´¢å¼•ï¼ˆä»… full æ¨¡å¼ï¼‰
        BuildDossier->>Indexer: åˆ›å»ºæ–‡æ¡£ç´¢å¼•
        loop æ¯ä¸ª normalized æ–‡ä»¶
            Indexer->>FileSystem: è¯»å– Markdown
            Indexer->>Indexer: åˆ†å—å¤„ç†
            Indexer->>FileSystem: å†™å…¥ index/documents.jsonl
        end
    end
    
    Note over BuildDossier: 7. å®Œæˆ
    BuildDossier->>FileSystem: æ›´æ–° manifest.json
    BuildDossier-->>User: è¿”å›ç»“æœå’Œç»Ÿè®¡
```

### ä¸‹è½½æ¨¡å¼å†³ç­–

ç³»ç»Ÿæ”¯æŒä¸¤ç§ä¸‹è½½æ¨¡å¼ï¼š

| æ¨¡å¼ | ä½¿ç”¨åœºæ™¯ | è¾“å‡º | æ€§èƒ½ |
|------|---------|------|------|
| `links_only` | åœ¨çº¿ç ”ç©¶ã€AI åˆ†æã€å¿«é€ŸæŸ¥è¯¢ | å…ƒæ•°æ® + SEC URL | æå¿«ï¼ˆæ— ä¸‹è½½ï¼‰ |
| `full` | ç¦»çº¿åˆ†æã€è‡ªå®šä¹‰å¤„ç†ã€åˆè§„å½’æ¡£ | å®Œæ•´æ–‡ä»¶ + è§„èŒƒåŒ– + ç´¢å¼• | è¾ƒæ…¢ï¼ˆéœ€ä¸‹è½½ï¼‰ |

### æ–‡ä»¶ä¸‹è½½æµç¨‹

```mermaid
flowchart TD
    Start([å¼€å§‹å¤„ç† filing]) --> CheckMode{æ£€æŸ¥ download_mode}
    
    CheckMode -->|links_only| GenerateURL[ç”Ÿæˆ SEC åœ¨çº¿é“¾æ¥]
    GenerateURL --> CreateMetadata[åˆ›å»ºå…ƒæ•°æ® artifact]
    CreateMetadata --> SetLinksOnly["è®¾ç½® parse_status = 'links_only'"]
    SetLinksOnly --> Done[å®Œæˆ]
    
    CheckMode -->|full| CheckCache{æ£€æŸ¥ç¼“å­˜}
    CheckCache -->|å­˜åœ¨ä¸”æœªè¿‡æœŸ| SkipDownload[è·³è¿‡ä¸‹è½½]
    CheckCache -->|ä¸å­˜åœ¨æˆ–è¿‡æœŸ| DetermineCategory[ç¡®å®šæ–‡ä»¶ç±»åˆ«]
    
    DetermineCategory --> CategoryMap{è¡¨å•ç±»å‹æ˜ å°„}
    CategoryMap -->|10-K/10-Q| FinancialReports[financial_reports/]
    CategoryMap -->|8-K| MaterialEvents[material_events/]
    CategoryMap -->|DEF 14A| ProxyStatements[proxy_statements/]
    CategoryMap -->|Form 4| InsiderTransactions[insider_transactions/]
    CategoryMap -->|å…¶ä»–| OtherFilings[other_filings/]
    
    FinancialReports --> BuildFilename[æ„å»ºæ–‡ä»¶å]
    MaterialEvents --> BuildFilename
    ProxyStatements --> BuildFilename
    InsiderTransactions --> BuildFilename
    OtherFilings --> BuildFilename
    
    BuildFilename --> FilenameFormat["æ ¼å¼: {date}_{form}_{accession}.ext"]
    FilenameFormat --> DownloadFile[ä¸‹è½½æ–‡ä»¶]
    
    DownloadFile --> CalculateHash[è®¡ç®— SHA256]
    CalculateHash --> SaveMetadata[ä¿å­˜å…ƒæ•°æ®]
    
    SaveMetadata --> UpdateManifest[æ›´æ–° Manifest]
    SkipDownload --> UpdateManifest
    
    UpdateManifest --> End([ç»“æŸ])
```

---

## æ ¸å¿ƒæ•°æ®æ¨¡å‹

### Artifact æ•°æ®ç»“æ„

```python
@dataclass
class Artifact:
    id: str                              # å”¯ä¸€æ ‡è¯†ç¬¦
    source: str                          # "sec" æˆ– "ir"
    type: str                            # "filing", "exhibit", "xbrl", etc.
    form: Optional[str] = None           # è¡¨å•ç±»å‹ (10-K, 10-Q, etc.)
    period: Optional[str] = None         # æŠ¥å‘ŠæœŸ
    filed_at: Optional[str] = None       # æäº¤æ—¥æœŸ
    url: str = ""                        # æº URL
    local_path: str = ""                 # æœ¬åœ°è·¯å¾„
    content_type: str = ""               # MIME ç±»å‹
    size_bytes: int = 0                  # æ–‡ä»¶å¤§å°
    sha256: str = ""                     # æ–‡ä»¶å“ˆå¸Œ
    downloaded_at: str = ""              # ä¸‹è½½æ—¶é—´
    parse_status: str = "pending"        # å¤„ç†çŠ¶æ€
    parse_error: Optional[str] = None    # é”™è¯¯ä¿¡æ¯
    versioning: Optional[Dict] = None    # ç‰ˆæœ¬æ§åˆ¶ä¿¡æ¯
```

**è®¾è®¡åŸç†**:
- **å¯è¿½æº¯æ€§**: æ¯ä¸ªå­—æ®µéƒ½æ”¯æŒå®¡è®¡è¿½è¸ª
- **å¹‚ç­‰æ€§**: SHA256 å“ˆå¸Œç¡®ä¿å†…å®¹ä¸å˜æ€§
- **çŠ¶æ€ç®¡ç†**: `parse_status` æ”¯æŒå·¥ä½œæµæ§åˆ¶

### Artifact çŠ¶æ€æœº

```mermaid
stateDiagram-v2
    [*] --> pending: åˆ›å»º Artifact
    
    pending --> success: ä¸‹è½½å¹¶å¤„ç†æˆåŠŸ
    pending --> failed: ä¸‹è½½æˆ–å¤„ç†å¤±è´¥
    pending --> skipped: å·²ç¼“å­˜ï¼Œè·³è¿‡
    
    success --> [*]
    failed --> [*]
    skipped --> [*]
    
    note right of success
        parse_status = "success"
        local_path æœ‰æ•ˆ
        sha256 å·²è®¡ç®—
    end note
    
    note right of failed
        parse_status = "failed"
        parse_error åŒ…å«é”™è¯¯ä¿¡æ¯
    end note
```

### Manifest ç»“æ„

```json
{
  "company": {
    "ticker": "TSLA",
    "cik": "0001318605",
    "company_name": "Tesla, Inc."
  },
  "run_info": {
    "run_id": "run_1770705802",
    "started_at": "2026-02-10T14:43:22.032565",
    "ended_at": "2026-02-10T14:43:26.204744",
    "status": "completed",
    "version": "1.0"
  },
  "config_snapshot": {
    "years": 2,
    "forms": ["10-K", "10-Q", "8-K"],
    "max_filings_per_form": 5,
    "normalize_level": "light",
    "sec_rps_limit": 3
  },
  "artifacts": [
    { /* Artifact objects */ }
  ]
}
```

**è®¾è®¡ä¼˜åŠ¿**:
- **å®Œæ•´æ€§**: æ•è·æ‰€æœ‰è¿è¡Œå‚æ•°
- **å¯é‡ç°æ€§**: å¯ä»¥ç²¾ç¡®é‡ç°ä»»ä½•å†å²è¿è¡Œ
- **ç‰ˆæœ¬æ§åˆ¶**: æ”¯æŒæœªæ¥æ¶æ„å‡çº§

---

## æ–‡ä»¶ç»„ç»‡ç­–ç•¥

### ç›®å½•ç»“æ„

```
dossiers/{TICKER}/
â”œâ”€â”€ manifest.json                    # ä¸»æ¸…å•æ–‡ä»¶
â”œâ”€â”€ raw/                            # åŸå§‹æ–‡ä»¶
â”‚   â”œâ”€â”€ sec/
â”‚   â”‚   â”œâ”€â”€ financial_reports/      # 10-K, 10-Q, 20-F, 40-F
â”‚   â”‚   â”œâ”€â”€ material_events/        # 8-K, 6-K
â”‚   â”‚   â”œâ”€â”€ proxy_statements/       # DEF 14A
â”‚   â”‚   â”œâ”€â”€ insider_transactions/   # Form 3, 4, 5
â”‚   â”‚   â”œâ”€â”€ registration_statements/# S-1, S-3, S-4, S-8
â”‚   â”‚   â”œâ”€â”€ institutional_holdings/ # 13F-HR, 13F-NT
â”‚   â”‚   â”œâ”€â”€ fund_reports/           # N-CSR, N-CSRS, N-Q
â”‚   â”‚   â”œâ”€â”€ other_filings/          # å…¶ä»–ç±»å‹
â”‚   â”‚   â””â”€â”€ structured_data/
â”‚   â”‚       â””â”€â”€ xbrl/               # XBRL JSON æ•°æ®
â”‚   â””â”€â”€ ir/
â”‚       â”œâ”€â”€ press_releases/         # æ–°é—»ç¨¿
â”‚       â”œâ”€â”€ presentations/          # æŠ•èµ„è€…æ¼”ç¤º
â”‚       â””â”€â”€ earnings/               # è´¢æŠ¥ææ–™
â”œâ”€â”€ normalized/                     # è§„èŒƒåŒ–æ–‡ä»¶
â”‚   â”œâ”€â”€ sec/
â”‚   â”‚   â””â”€â”€ {same structure as raw}
â”‚   â””â”€â”€ ir/
â”‚       â””â”€â”€ {same structure as raw}
â”œâ”€â”€ index/
â”‚   â””â”€â”€ documents.jsonl             # å…¨æ–‡ç´¢å¼•
â””â”€â”€ logs/
    â””â”€â”€ run_*.log                   # æ‰§è¡Œæ—¥å¿—
```

### æ–‡ä»¶å‘½åè§„åˆ™

**æ ¼å¼**: `{filed_date}_{form_type}_{accession_number}.{ext}`

**ç¤ºä¾‹**: `2026-01-29_10-K_000162828026003952.htm`

**è®¾è®¡åŸç†**:
1. **æ—¶é—´æ’åº**: æ–‡ä»¶åä»¥æ—¥æœŸå¼€å¤´ï¼Œä¾¿äºæŒ‰æ—¶é—´æ’åº
2. **ç±»å‹è¯†åˆ«**: è¡¨å•ç±»å‹æ¸…æ™°å¯è§
3. **å”¯ä¸€æ€§ä¿è¯**: accession number ç¡®ä¿åŒä¸€å¤©å¤šä¸ªæ–‡ä»¶ä¸å†²çª
4. **æ‰©å±•åä¿ç•™**: ä¿æŒåŸå§‹æ–‡ä»¶æ ¼å¼ä¿¡æ¯

### åˆ†ç±»æ˜ å°„è¡¨

| SEC è¡¨å•ç±»å‹ | ç›®å½•åˆ†ç±» | è¯´æ˜ |
|-------------|---------|------|
| 10-K, 10-Q, 20-F, 40-F | `financial_reports/` | å®šæœŸè´¢åŠ¡æŠ¥å‘Š |
| 8-K, 6-K | `material_events/` | é‡å¤§äº‹ä»¶æŠ«éœ² |
| DEF 14A | `proxy_statements/` | å§”æ‰˜ä¹¦/ä»£ç†å£°æ˜ |
| 3, 4, 5 | `insider_transactions/` | å†…éƒ¨äººäº¤æ˜“ |
| S-1, S-3, S-4, S-8 | `registration_statements/` | æ³¨å†Œå£°æ˜ |
| 13F-HR, 13F-NT | `institutional_holdings/` | æœºæ„æŒä»“ |
| N-CSR, N-CSRS, N-Q | `fund_reports/` | åŸºé‡‘æŠ¥å‘Š |

---

## å…³é”®ç®—æ³•è¯¦è§£

### 1. ä¸‹è½½æ¨¡å¼å¤„ç†ç®—æ³•

**é—®é¢˜**: å¦‚ä½•åœ¨ `links_only` å’Œ `full` æ¨¡å¼ä¹‹é—´é«˜æ•ˆåˆ‡æ¢ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**: åœ¨æ–‡ä»¶ä¸‹è½½å¾ªç¯ä¸­ä½¿ç”¨æ—©æœŸè¿”å›ï¼ˆearly returnï¼‰æ¨¡å¼

```python
def _download_sec_filings_optimized(self, ticker, cik, filings, dossier_path, 
                                    force_rebuild, download_mode="links_only"):
    """æ”¯æŒä¸¤ç§ä¸‹è½½æ¨¡å¼çš„æ–‡ä»¶å¤„ç†"""
    artifacts = []
    
    for filing in filings:
        # 1. Links-only æ¨¡å¼ï¼šç”Ÿæˆé“¾æ¥å¹¶æ—©æœŸè¿”å›
        if download_mode == "links_only":
            viewer_url = self._generate_online_viewer_url(
                cik, filing["accession_number"], primary_doc
            )
            artifact = Artifact(
                id=artifact_id,
                url=primary_url,
                local_path="",  # ç©ºè·¯å¾„è¡¨ç¤ºæ— æœ¬åœ°æ–‡ä»¶
                parse_status="links_only",
                metadata={"viewer_url": viewer_url, "raw_url": primary_url}
            )
            artifacts.append(artifact)
            continue  # è·³è¿‡æ–‡ä»¶ä¸‹è½½é€»è¾‘
        
        # 2. Full æ¨¡å¼ï¼šæ£€æŸ¥ç¼“å­˜
        if not force_rebuild and os.path.exists(local_path):
            # ä½¿ç”¨å·²æœ‰æ–‡ä»¶
            artifact = Artifact(..., local_path=local_path, ...)
            artifacts.append(artifact)
            continue
        
        # 3. Full æ¨¡å¼ï¼šä¸‹è½½æ–‡ä»¶
        response = requests.get(primary_url, headers=headers)
        with open(local_path, "wb") as f:
            f.write(response.content)
        
        artifact = Artifact(..., local_path=local_path, ...)
        artifacts.append(artifact)
    
    return artifacts
```

**è®¾è®¡å†³ç­–**:
- âœ… **æ€§èƒ½ä¼˜åŒ–**: `links_only` æ¨¡å¼é€šè¿‡ `continue` å®Œå…¨è·³è¿‡ä¸‹è½½é€»è¾‘
- âœ… **ä»£ç å¤ç”¨**: ä¸¤ç§æ¨¡å¼å…±äº« filing å¾ªç¯å’Œå…ƒæ•°æ®æ„å»º
- âœ… **æ¸…æ™°åˆ†ç¦»**: æ¨¡å¼æ£€æŸ¥åœ¨å¾ªç¯é¡¶éƒ¨ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤
- âœ… **é›¶å¼€é”€**: `links_only` æ¨¡å¼æ— æ–‡ä»¶ I/Oï¼Œæå¿«æ‰§è¡Œ

**ç”Ÿæˆåœ¨çº¿æŸ¥çœ‹å™¨ URL**:
```python
def _generate_online_viewer_url(self, cik, accession_number, primary_document):
    """ç”Ÿæˆ SEC å®˜æ–¹åœ¨çº¿æŸ¥çœ‹å™¨é“¾æ¥"""
    return f"https://www.sec.gov/cgi-bin/viewer?action=view&cik={cik}&accession_number={accession_number}&xbrl_type=v"
```

**Manifest è¾“å‡ºå·®å¼‚**:

Links-only æ¨¡å¼çš„ artifact:
```json
{
  "id": "sec_filing_0001318605-23-000012_10-K",
  "url": "https://www.sec.gov/Archives/edgar/data/1318605/...",
  "local_path": "",
  "parse_status": "links_only",
  "metadata": {
    "viewer_url": "https://www.sec.gov/cgi-bin/viewer?action=view&cik=1318605&accession_number=0001318605-23-000012&xbrl_type=v",
    "raw_url": "https://www.sec.gov/Archives/edgar/data/1318605/..."
  }
}
```

Full æ¨¡å¼çš„ artifact:
```json
{
  "id": "sec_filing_0001318605-23-000012_10-K",
  "url": "https://www.sec.gov/Archives/edgar/data/1318605/...",
  "local_path": "dossiers/TSLA/raw/sec/financial_reports/2023-01-31_10-K_0001318605230000012.htm",
  "parse_status": "pending",
  "sha256": "abc123...",
  "size_bytes": 1234567
}
```

### 2. è¡¨å•ç±»å‹è®¡æ•°ç®—æ³•

**é—®é¢˜**: å¦‚ä½•ç¡®ä¿æ¯ç§è¡¨å•ç±»å‹éƒ½è·å–æ­£ç¡®æ•°é‡çš„æ–‡ä»¶ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ç‹¬ç«‹è®¡æ•°å™¨è·Ÿè¸ªæ¯ç§è¡¨å•ç±»å‹

```python
def _fetch_sec_filings(self, cik, forms, years, max_filings_per_form):
    """æ™ºèƒ½è¡¨å•è®¡æ•°ç®—æ³•"""
    filtered_filings = []
    form_counts = {form: 0 for form in forms}  # ä¸ºæ¯ç§ç±»å‹åˆå§‹åŒ–è®¡æ•°å™¨
    
    for i, form in enumerate(all_filings["form"]):
        if form not in forms:
            continue
        
        # æ£€æŸ¥è¯¥ç±»å‹æ˜¯å¦å·²è¾¾ä¸Šé™
        if form_counts[form] >= max_filings_per_form:
            continue  # è·³è¿‡ï¼Œä¸ä¸­æ–­å¾ªç¯
        
        # æ—¥æœŸè¿‡æ»¤
        if filed_date < cutoff_date:
            continue
        
        # æ·»åŠ æ–‡ä»¶å¹¶å¢åŠ è®¡æ•°
        filtered_filings.append(filing)
        form_counts[form] += 1
    
    return filtered_filings
```

**ä¼˜åŠ¿**:
- âœ… ç¡®ä¿æ¯ç§ç±»å‹éƒ½èƒ½è·å–æŒ‡å®šæ•°é‡
- âœ… é¿å…æŸç±»å‹è¿‡åº¦é‡‡é›†å¯¼è‡´å…¶ä»–ç±»å‹ç¼ºå¤±
- âœ… O(n) æ—¶é—´å¤æ‚åº¦ï¼Œé«˜æ•ˆ

**é”™è¯¯å®ç°å¯¹æ¯”**:
```python
# âŒ é”™è¯¯ï¼šæŒ‰æ€»æ•°é™åˆ¶ä¼šå¯¼è‡´ç±»å‹åˆ†å¸ƒä¸å‡
if len(filtered_filings) >= max_filings_per_form:
    break  # æå‰ç»ˆæ­¢ï¼Œå…¶ä»–ç±»å‹å¯èƒ½ç¼ºå¤±
```

### 3. Normalized æ–‡ä»¶è·¯å¾„è§£æç®—æ³•

**é—®é¢˜**: å¦‚ä½•ä»åŸå§‹æ–‡ä»¶è·¯å¾„æ­£ç¡®æ„å»º normalized æ–‡ä»¶è·¯å¾„ï¼Ÿ

**æŒ‘æˆ˜**: 
- åŸå§‹è·¯å¾„: `/path/to/dossiers/TSLA/raw/sec/financial_reports/file.htm`
- Normalized è·¯å¾„: `/path/to/dossiers/TSLA/normalized/sec/financial_reports/artifact_id.md`

**è§£å†³æ–¹æ¡ˆ**:

```python
def _chunk_document(self, artifact, normalized_artifact, normalized_path):
    """æ­£ç¡®çš„è·¯å¾„æ„å»ºç®—æ³•"""
    # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äº raw æ ¹ç›®å½•ï¼‰
    relative_path = os.path.relpath(
        os.path.dirname(artifact.local_path),
        os.path.join(os.path.dirname(artifact.local_path), '..', '..')
    )
    # ç¤ºä¾‹: "sec/financial_reports"
    
    # æ„å»ºå®Œæ•´ normalized è·¯å¾„
    normalized_dir = os.path.join(normalized_path, relative_path)
    normalized_md_path = os.path.join(normalized_dir, f"{artifact.id}.md")
    
    return normalized_md_path
```

**è·¯å¾„è½¬æ¢ç¤ºä¾‹**:
```
Input:  /dossiers/TSLA/raw/sec/financial_reports/2026-01-29_10-K.htm
                           â†“ ç›¸å¯¹è·¯å¾„è®¡ç®—
Relative: sec/financial_reports
                           â†“ æ‹¼æ¥ normalized_path
Output: /dossiers/TSLA/normalized/sec/financial_reports/artifact_123.md
```

### 3. æ–‡æ¡£åˆ†å—ç®—æ³•

**ç›®æ ‡**: å°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚åˆ RAG æ£€ç´¢çš„è¯­ä¹‰å—

```python
def _chunk_document(self, artifact, normalized_artifact, normalized_path):
    """åŸºäºæ ‡é¢˜çš„æ–‡æ¡£åˆ†å—"""
    lines = content.split('\n')
    chunks = []
    current_chunk = []
    current_section = []
    
    for line in lines:
        if line.strip().startswith('#'):  # Markdown æ ‡é¢˜
            # é‡åˆ°æ–°æ ‡é¢˜ï¼Œä¿å­˜å½“å‰å—
            if current_chunk:
                chunk_text = '\n'.join(current_chunk)
                if len(chunk_text.strip()) > 50:  # æœ€å°å—å¤§å°
                    chunks.append({
                        "artifact_id": artifact.id,
                        "source_url": artifact.url,
                        "section_path": current_section.copy(),
                        "chunk_index": len(chunks),
                        "text": chunk_text,
                        "word_count": len(chunk_text.split())
                    })
            current_chunk = []
            current_section = [line.strip()]
        else:
            current_chunk.append(line)
    
    # ä¿å­˜æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunk_text = '\n'.join(current_chunk)
        if len(chunk_text.strip()) > 50:
            chunks.append({...})
    
    return chunks
```

**åˆ†å—ç­–ç•¥**:
- **è§¦å‘æ¡ä»¶**: Markdown æ ‡é¢˜ (`#`, `##`, `###`)
- **æœ€å°å—å¤§å°**: 50 å­—ç¬¦ï¼ˆè¿‡æ»¤ç©ºå—å’Œè¶…çŸ­å—ï¼‰
- **å…ƒæ•°æ®ä¿ç•™**: æ¯ä¸ªå—åŒ…å«å®Œæ•´è¿½æº¯ä¿¡æ¯

**è¾“å‡ºæ ¼å¼** (JSONL):
```json
{"artifact_id": "sec_filing_123", "source_url": "https://...", "text": "...", "word_count": 597}
{"artifact_id": "sec_filing_456", "source_url": "https://...", "text": "...", "word_count": 1234}
```

### 4. å¢é‡æ›´æ–°ç­–ç•¥

**ç›®æ ‡**: é¿å…é‡å¤ä¸‹è½½æœªæ”¹å˜çš„æ–‡ä»¶

```python
def _download_sec_filings_optimized(self, ..., force_rebuild):
    """æ™ºèƒ½ç¼“å­˜æ£€æŸ¥"""
    if not force_rebuild and os.path.exists(local_path):
        # è®¡ç®—ç°æœ‰æ–‡ä»¶å“ˆå¸Œ
        existing_hash = self._calculate_file_hash(local_path)
        
        # åˆ›å»º artifactï¼ˆè·³è¿‡ä¸‹è½½ï¼‰
        artifact = Artifact(
            ...,
            sha256=existing_hash,
            downloaded_at=os.path.getmtime(local_path),
            parse_status="pending"
        )
        return artifact  # ä½¿ç”¨ç¼“å­˜
    
    # ä¸‹è½½æ–°æ–‡ä»¶...
```

**ç¼“å­˜åˆ¤æ–­æµç¨‹**:

```mermaid
flowchart TD
    Start([æ£€æŸ¥æ–‡ä»¶]) --> ForceRebuild{force_rebuild?}
    
    ForceRebuild -->|æ˜¯| Download[ä¸‹è½½æ–‡ä»¶]
    ForceRebuild -->|å¦| FileExists{æ–‡ä»¶å­˜åœ¨?}
    
    FileExists -->|å¦| Download
    FileExists -->|æ˜¯| CalculateHash[è®¡ç®—ç°æœ‰æ–‡ä»¶å“ˆå¸Œ]
    
    CalculateHash --> CompareHash{å“ˆå¸ŒåŒ¹é…?}
    CompareHash -->|æ˜¯| UseCache[ä½¿ç”¨ç¼“å­˜]
    CompareHash -->|å¦| Download
    
    UseCache --> End([è¿”å›])
    Download --> End
```

---

## åˆè§„æ€§è®¾è®¡

### SEC API é€Ÿç‡é™åˆ¶

**SEC è§„å®š**: æœ€å¤š 10 requests/second

**å®ç°ç­–ç•¥**:
```python
self.sec_rps_limit = int(os.getenv("SEC_RPS_LIMIT", "3"))  # é»˜è®¤ä¿å®ˆå€¼
```

**é€Ÿç‡æ§åˆ¶æµç¨‹** (å»ºè®®å®ç°):
```python
import time
from collections import deque

class RateLimiter:
    def __init__(self, max_rps=3):
        self.max_rps = max_rps
        self.requests = deque()
    
    def wait_if_needed(self):
        now = time.time()
        # ç§»é™¤ 1 ç§’å‰çš„è¯·æ±‚
        while self.requests and self.requests[0] < now - 1:
            self.requests.popleft()
        
        # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œç­‰å¾…
        if len(self.requests) >= self.max_rps:
            sleep_time = 1 - (now - self.requests[0])
            if sleep_time > 0:
                time.sleep(sleep_time)
        
        self.requests.append(time.time())
```

### User-Agent è¦æ±‚

**SEC è§„å®š**: å¿…é¡»åŒ…å«è”ç³»ä¿¡æ¯

```python
# âœ… æ­£ç¡®æ ¼å¼
SEC_USER_AGENT="MyResearchBot/1.0 (contact@example.com)"

# âŒ é”™è¯¯æ ¼å¼ï¼ˆä¼šè¢«æ‹’ç»ï¼‰
SEC_USER_AGENT="Mozilla/5.0 ..."  # ä¼ªè£…æµè§ˆå™¨
```

**éªŒè¯æœºåˆ¶**:
```python
def validate_user_agent(user_agent):
    if not re.search(r'\([^)]*@[^)]*\)', user_agent):
        raise ValueError("User-Agent must contain contact email")
```

### é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
def download_with_retry(url, max_retries=3, backoff_factor=2):
    """æŒ‡æ•°é€€é¿é‡è¯•"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:  # Rate limited
                wait_time = backoff_factor ** attempt
                logger.warning(f"Rate limited, waiting {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
    raise Exception(f"Failed after {max_retries} retries")
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. ä¸‹è½½æ¨¡å¼æ€§èƒ½å¯¹æ¯”

**Links-Only æ¨¡å¼æ€§èƒ½ä¼˜åŠ¿**:
- âš¡ **æ‰§è¡Œé€Ÿåº¦**: æ¯” full æ¨¡å¼å¿« 50-100 å€ï¼ˆæ— æ–‡ä»¶ä¸‹è½½ï¼‰
- ğŸ’¾ **å­˜å‚¨ç©ºé—´**: èŠ‚çœ 99% ç£ç›˜ç©ºé—´ï¼ˆä»…å­˜å‚¨å…ƒæ•°æ®ï¼‰
- ğŸŒ **å¸¦å®½ä½¿ç”¨**: å‡å°‘ 99% ç½‘ç»œæµé‡ï¼ˆä»… API è¯·æ±‚ï¼‰
- â±ï¸ **å“åº”æ—¶é—´**: å…¸å‹å…¬å¸ 3 å¹´æ•°æ® < 5 ç§’ï¼ˆvs. full æ¨¡å¼ 3-5 åˆ†é’Ÿï¼‰

**æ€§èƒ½æµ‹è¯•æ•°æ®** (TSLA, 3 years, 10-K + 10-Q + 8-K):

| æ¨¡å¼ | æ‰§è¡Œæ—¶é—´ | ç£ç›˜å ç”¨ | ç½‘ç»œæµé‡ | æ–‡ä»¶æ•° |
|------|---------|---------|---------|--------|
| `links_only` | 4.2s | 45 KB | 120 KB | 0 |
| `full` | 3m 45s | 125 MB | 126 MB | 48 |

**ä½¿ç”¨å»ºè®®**:
- ğŸ¯ **é»˜è®¤ä½¿ç”¨** `links_only` æ¨¡å¼æ»¡è¶³ 95% ä½¿ç”¨åœºæ™¯
- ğŸ“Š æ•°æ®åˆ†æã€è´¢åŠ¡ç ”ç©¶ã€AI é©±åŠ¨çš„å·¥ä½œæµå‡å¯ä½¿ç”¨ `links_only`
- ğŸ’¿ ä»…åœ¨éœ€è¦ç¦»çº¿è®¿é—®æˆ–è‡ªå®šä¹‰å¤„ç†æ—¶ä½¿ç”¨ `full` æ¨¡å¼

### 2. å†…å­˜ç®¡ç†

**å¤§æ–‡ä»¶æµå¼å¤„ç†**:
```python
def _calculate_file_hash(self, filepath):
    """æµå¼è®¡ç®—å“ˆå¸Œï¼Œé¿å…å…¨éƒ¨åŠ è½½åˆ°å†…å­˜"""
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()
```

### 3. å¹¶å‘ä¸‹è½½ï¼ˆæœªå®ç°ï¼Œè®¾è®¡æ€è·¯ï¼‰

```python
from concurrent.futures import ThreadPoolExecutor

def download_filings_parallel(filings, max_workers=5):
    """å¹¶å‘ä¸‹è½½å¤šä¸ªæ–‡ä»¶"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(download_filing, filing)
            for filing in filings
        ]
        results = [f.result() for f in futures]
    return results
```

**æ³¨æ„äº‹é¡¹**:
- å¿…é¡»é…åˆé€Ÿç‡é™åˆ¶å™¨ä½¿ç”¨
- å»ºè®® max_workers <= sec_rps_limit

### 4. ç¼“å­˜ä¼˜åŒ–

**Ticker-CIK æ˜ å°„ç¼“å­˜**:
```python
self.ticker_cik_map = {}  # å†…å­˜ç¼“å­˜

# ç¬¬ä¸€æ¬¡æŸ¥è¯¢åç¼“å­˜åˆ°æ–‡ä»¶
with open("ticker_cik_map.json", "w") as f:
    json.dump(self.ticker_cik_map, f)
```

---

## å¯æ‰©å±•æ€§è®¾è®¡

### 1. æ’ä»¶åŒ–è¡¨å•å¤„ç†

**å½“å‰å®ç°**: ç¡¬ç¼–ç è¡¨å•ç±»å‹æ˜ å°„

**æ‰©å±•è®¾è®¡**:
```python
class FormProcessor(ABC):
    @abstractmethod
    def can_handle(self, form_type: str) -> bool:
        pass
    
    @abstractmethod
    def process(self, artifact: Artifact) -> Artifact:
        pass

# æ³¨å†Œå¤„ç†å™¨
processors = [
    FinancialReportProcessor(),
    MaterialEventProcessor(),
    CustomProcessor()
]
```

### 2. å¤šæ•°æ®æºæ”¯æŒ

**æ¥å£è®¾è®¡**:
```python
class DataSource(ABC):
    @abstractmethod
    def fetch_filings(self, ticker, filters):
        pass
    
    @abstractmethod
    def download_file(self, filing):
        pass

# å®ç°
sources = {
    "sec": SECDataSource(),
    "edgar": EdgarDataSource(),
    "custom": CustomDataSource()
}
```

### 3. è‡ªå®šä¹‰ç´¢å¼•ç­–ç•¥

```python
class ChunkingStrategy(ABC):
    @abstractmethod
    def chunk(self, content: str) -> List[Dict]:
        pass

# ç­–ç•¥é€‰æ‹©
strategies = {
    "heading": HeadingBasedChunking(),
    "semantic": SemanticChunking(),
    "fixed": FixedSizeChunking(size=1000)
}
```

---

## æŠ€æœ¯å€ºåŠ¡å’Œæœªæ¥æ”¹è¿›

### å½“å‰é™åˆ¶

1. **PDF è§£ææœªå®ç°**
   - ç°çŠ¶: PDF æ–‡ä»¶æ ‡è®°ä¸º "failed"
   - éœ€è¦: é›†æˆ pdfplumber æˆ– PyMuPDF

2. **IR ç½‘ç«™çˆ¬å–æœªå®Œæˆ**
   - ç°çŠ¶: ä»…åˆ›å»ºç›®å½•ç»“æ„
   - éœ€è¦: å®ç°é€šç”¨çˆ¬è™«æˆ–ç‰¹å®šç«™ç‚¹é€‚é…å™¨

3. **æ–‡æ¡£åˆ†å—è¾ƒç®€å•**
   - ç°çŠ¶: åŸºäº Markdown æ ‡é¢˜
   - æ”¹è¿›: è¯­ä¹‰åˆ†å—ã€é‡å å—ã€æ™ºèƒ½åˆ†æ®µ

4. **æ— å¹¶å‘ä¸‹è½½**
   - ç°çŠ¶: ä¸²è¡Œä¸‹è½½
   - æ”¹è¿›: çº¿ç¨‹æ± æˆ–å¼‚æ­¥ IO

### æœªæ¥è·¯çº¿å›¾

#### Phase 1: å®Œå–„æ ¸å¿ƒåŠŸèƒ½
- [ ] å®ç° PDF è§£æ
- [ ] å®Œå–„ IR ç«™ç‚¹çˆ¬å–
- [ ] æ·»åŠ è¿›åº¦æ¡å’Œè¯¦ç»†æ—¥å¿—

#### Phase 2: æ€§èƒ½ä¼˜åŒ–
- [ ] å¹¶å‘ä¸‹è½½ï¼ˆçº¿ç¨‹æ± ï¼‰
- [ ] æ›´æ™ºèƒ½çš„ç¼“å­˜ç­–ç•¥
- [ ] å¢é‡æ›´æ–°ä¼˜åŒ–

#### Phase 3: é«˜çº§åŠŸèƒ½
- [ ] è¯­ä¹‰æ–‡æ¡£åˆ†å—
- [ ] å‘é‡ç´¢å¼•æ”¯æŒï¼ˆfor RAGï¼‰
- [ ] å¤šè¯­è¨€æ”¯æŒ

#### Phase 4: ä¼ä¸šçº§ç‰¹æ€§
- [ ] åˆ†å¸ƒå¼å¤„ç†ï¼ˆCeleryï¼‰
- [ ] æ•°æ®åº“åç«¯ï¼ˆPostgreSQLï¼‰
- [ ] Web ç®¡ç†ç•Œé¢
- [ ] API æœåŠ¡

---

## è´¨é‡ä¿è¯

### æ•°æ®å®Œæ•´æ€§éªŒè¯

```python
def _calculate_quality_metrics(self, manifest, years):
    """è´¨é‡æŒ‡æ ‡è®¡ç®—"""
    return {
        "completeness": {
            "has_recent_10k": form_counts.get("10-K") >= min(years, 3),
            "has_recent_10q": form_counts.get("10-Q") >= min(years * 4, 8),
            "has_xbrl": any(a["type"] == "xbrl" for a in artifacts)
        },
        "freshness": {
            "latest_filed_at": latest_filed,
            "days_since_last_update": days_diff
        },
        "traceability": {
            "all_artifacts_have_ids": all(a.get("id") for a in artifacts),
            "all_artifacts_have_urls": all(a.get("url") for a in artifacts)
        }
    }
```

### æµ‹è¯•ç­–ç•¥

**å•å…ƒæµ‹è¯•** (å»ºè®®):
```python
def test_form_type_counting():
    """æµ‹è¯•è¡¨å•ç±»å‹è®¡æ•°ç®—æ³•"""
    filings = [
        {"form": "10-K", "date": "2024-01-01"},
        {"form": "10-K", "date": "2024-02-01"},
        {"form": "10-Q", "date": "2024-03-01"},
    ]
    result = fetch_sec_filings(cik, forms=["10-K", "10-Q"], max=1)
    assert count_by_form(result) == {"10-K": 1, "10-Q": 1}
```

**é›†æˆæµ‹è¯•**:
```bash
# ä½¿ç”¨çœŸå® API è¿›è¡Œå°è§„æ¨¡æµ‹è¯•
python cli.py build AAPL --years 1 --forms 10-K --max-filings-per-form 1
```

---

## éƒ¨ç½²å»ºè®®

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env æ–‡ä»¶
SEC_USER_AGENT="MyBot/1.0 (myemail@example.com)"
SEC_RPS_LIMIT=3
WORKSPACE_ROOT="/data/dossiers"
DOMAIN_ALLOWLIST="investor.apple.com,investor.tesla.com"
```

### ä¾èµ–ç®¡ç†

```bash
# requirements.txt
requests>=2.25.0
beautifulsoup4>=4.9.0
lxml>=4.6.0
html2text>=2020.1.16
pydantic>=1.8.0  # ç”¨äºæœªæ¥æ•°æ®éªŒè¯
```

### è¿è¡Œèµ„æºéœ€æ±‚

| èµ„æº | æœ€å° | æ¨è |
|------|------|------|
| CPU | 2 æ ¸ | 4 æ ¸ |
| å†…å­˜ | 2 GB | 8 GB |
| ç£ç›˜ | 10 GB | 100 GB+ |
| ç½‘ç»œ | 10 Mbps | 100 Mbps |

### ç›‘æ§æŒ‡æ ‡

```python
metrics = {
    "downloads_per_hour": 120,
    "success_rate": 0.95,
    "avg_file_size_mb": 1.2,
    "cache_hit_rate": 0.60,
    "api_error_rate": 0.02
}
```

---

## å‚è€ƒèµ„æº

### SEC API æ–‡æ¡£
- [SEC EDGAR API](https://www.sec.gov/edgar/sec-api-documentation)
- [Rate Limiting Policy](https://www.sec.gov/os/accessing-edgar-data)
- [XBRL API](https://www.sec.gov/dera/data/financial-statement-data-sets.html)

### ç›¸å…³æŠ€æœ¯
- [html2text](https://github.com/Alir3z4/html2text) - HTML è½¬ Markdown
- [Requests](https://requests.readthedocs.io/) - HTTP å®¢æˆ·ç«¯
- [dataclasses](https://docs.python.org/3/library/dataclasses.html) - æ•°æ®ç±»

---

## è´¡çŒ®æŒ‡å—

### ä»£ç é£æ ¼
- éµå¾ª PEP 8
- ä½¿ç”¨ç±»å‹æ³¨è§£
- ç¼–å†™ docstrings

### æäº¤æµç¨‹
1. Fork ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æ·»åŠ æµ‹è¯•
4. æäº¤ PR

### è”ç³»æ–¹å¼
- é—®é¢˜åé¦ˆ: GitHub Issues
- æŠ€æœ¯è®¨è®º: é‚®ä»¶åˆ—è¡¨

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2026-02-10  
**ç»´æŠ¤è€…**: OpenClaw Team
